{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 - Implementing epsilon-greedy policy\n",
    "\n",
    "This lab re-uses concepts and components from Lab 02 and Lab 03.\n",
    "\n",
    "In order to use model-free control techniques, we learn the Q-values either trough MC-learning or TD-learning.\n",
    "The environment is explored using an epsilon-greedy policy.\n",
    "\n",
    "Implement a policy class that selects actions within the probabilities of actions given values, using the e-greedy approach:\n",
    "- epsilon is an attribute\n",
    "- add a method to decrease the value of epsilon (this method might depend on a hyperparameter)\n",
    "- add a method to sample the action based on the q-values\n",
    "- the instance of the class returns a sampled action when called.\n",
    "\n",
    "We will use the Ice dungeon developped during the last lab.\n",
    "You can use your own or use the one provided in this lab.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class E_Greedy_Policy():\n",
    "    \n",
    "    def __init__(self, env, epsilon, decay):\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.decay = decay\n",
    "        \n",
    "        \n",
    "    def __call__(self, state, q_values):\n",
    "        # Sample an action from the policy, given a state\n",
    "        \n",
    "        ...        \n",
    "        return action\n",
    "        \n",
    "    def update_epsilon(self):\n",
    "        \n",
    "        self.epsilon = ...\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 - SARSA\n",
    "\n",
    "Now that we have our e-greedy policy, we need to refine our q-values little by little.\n",
    "\n",
    "Create a new class SARSA that inherits from the E-greedy policy, and implements a method to update Q-values based on TD-learning.\n",
    "\n",
    "Additionally, create a function to display the values learned. As you have 4 q-values per state, you can decide to display the values of the state when taking the best action.\n",
    "\n",
    "Once this is implemented, you can have your policy control the agent and observe how the q-values (when taking the greedy action) look like.\n",
    "\n",
    "You can also print the mean reward every k episodes, to see how the agent performs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA:\n",
    "    \n",
    "    def __init__(self, env, epsilon, decay):\n",
    "        \n",
    "        \n",
    "    def update_values(self, s_current, a_next, r_next, s_next, a_next):\n",
    "        ...\n",
    "        \n",
    "    def display_values(self):\n",
    "        ...\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: MC-control\n",
    "\n",
    "We will now compare MC-control with Sarsa.\n",
    "\n",
    "Follow the steps in Exercise 2 to create a MC-control that updates values based on rollout after each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC-control:\n",
    "    \n",
    "    def __init__(self, env, epsilon, decay):\n",
    "                \n",
    "    def update_values(self, s_current, a_next, r_next, s_next, a_next):\n",
    "        ...\n",
    "        \n",
    "    def display_values(self):\n",
    "        ...\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
